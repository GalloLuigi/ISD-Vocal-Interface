<!DOCTYPE html>
<html>
<head>
    <style>
        .piccolo {
            font-size: 12px;
        }
    </style>
    <!-- Set the character encoding to UTF-8 -->
    <meta charset="UTF-8">
    <!-- Specify the compatibility mode for Internet Explorer -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <!-- Set the viewport to control the layout on mobile devices -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Set the title of the web page -->
    <title>PDF voice notebook </title>
    <!-- Include the PDF.js library -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.4.120/pdf.min.js"
            integrity="sha512-ml/QKfG3+Yes6TwOzQb7aCNtJF4PUyha6R3w8pSTo/VJSywl7ZreYvvtUso7fKevpsI+pYVVwnu82YO0q3V6eg=="
            crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <!-- Add some styling for the web page -->
    <link rel="stylesheet" href="style.css">
</head>
<body>
<!-- Display the heading -->
<h1>PDF voice notebook:</h1>
<!-- Create a div container for the file upload form and result section -->
<div class="pdfwork">
    <!-- Button to extract another PDF (hidden initially) -->
    <button class="another" onclick="location.reload()">Extract Another PDF</button>
    <!-- Display text "Select PDF" -->
    <span>Select PDF</span>
    <!-- File input field for selecting the PDF file -->
    <input type="file" class="selectpdf">
    <!-- Display text "Password :" -->
    <span>Password :</span>
    <!-- Password input field (optional) -->
    <input type="password" class="pwd" placeholder='optional'>
    <!-- Button to upload the selected PDF -->
    <button class="upload">Upload</button>
    <!-- Result section (hidden initially) -->
    <div class="afterupload">
        <!-- Display text "Select Page" -->
        <span style="display: none;">Select Page</span>
        <!-- Dropdown menu for selecting the page -->
        <select class="selectpage" onchange="afterProcess()" style="display: none;"></select>
        <!-- Download link for the extracted text file -->
        <a href="" style="display: none;" class="download" download>Download Pdf Text</a>
        <!-- Textarea to display the extracted text -->
        <textarea style="display: none;" class="pdftext"></textarea>
    </div>
</div>


<div id="outputElement"></div>
<div class="container">
    <div id="target-div">
        <div>We present a novel voice-based humancomputer
            interface designed to enable individuals
            wit
        </div>
        <div>h motor impairments to use
            vocal parameters for continuous control
            tasks. Since discrete
        </div>
        <div>spoken commands
            are ill-suited to such tasks, our interface
            exploits a large set of conti
        </div>
        <div>nuous acousticphonetic
            parameters like pitch, loudness,
            vowel quality, etc. Their selecti
        </div>
        <div>on is optimized
            with respect to automatic recognizability,
            communication bandwidth, learn
        </div>
        <div>ability,
            suitability, and ease of use. Parameters
            are extracted in real time, transformed
        </div>
        <div>
            via adaptation and acceleration,
            and converted into continuous control signals.
            This pap
        </div>
        <div>er describes the basic engine,
            prototype applications (in particular,
            voice-based web bro
        </div>
        <div>wsing and a controlled
            trajectory-following task), and initial
            user studies confirming th
        </div>
        <div>e feasibility
            of this technology.
        </div>
        <div>Many existing human-computer interfaces (e.g.,
            mouse and keyboard, touch screens, pen tab
        </div>
        <div>lets,
            etc.) are ill-suited to individuals with motor
            impairments. Specialized (and often
        </div>
        <div>expensive)
            human-computer interfaces that have been developed
            specifically for this targe
        </div>
        <div>t group include sip
            and puff switches, head mice, eye-gaze devices, chin
            joysticks and to
        </div>
        <div>ngue switches. While many individuals
            with motor impairments have complete use of their v
        </div>
        <div>ocal system, these devices make little use
            of it. Sip and puff switches, for example, hav
        </div>
        <div>e low
            communication bandwidth, making it impossible to
            achieve more complex control tasks
        </div>
        <div>.
            Natural spoken language is often regarded as
            the obvious choice for a human-computer in
        </div>
        <div>terface.
            However, despite significant research efforts
            in automatic speech recognition (A
        </div>
        <div>SR) (Huang et
            al., 2001), existing ASR systems are still not sufficiently
            robust to a wid
        </div>
        <div>e variety of speaking conditions,
            noise, accented speakers, etc. ASR-based interfaces
            are
        </div>
        <div> therefore often abandoned by users after
            a short initial trial period. In addition, natu
        </div>
        <div>ral speech
            is optimal for communication between humans but
            sub-optimal for manipulating c
        </div>
        <div>omputers, windowsicons-
            mouse-pointer (WIMP) interfaces, or other
            electro-mechanical devi
        </div>
        <div>ces (such as a prosthetic robotic
            arm). Standard spoken language commands
            are useful for
        </div>
        <div>discrete but not for continuous operations.
            For example, in order to move a cursor
            from t
        </div>
        <div>he bottom-left to the upper-right of a screen,
            the user might have to repeatedly utter “u
        </div>
        <div>p” and
            “right” or “stop” and “go” after setting an initial trajectory
            and rate, which is
        </div>
        <div>quite inefficient. For these
            reasons, we are developing alternative and reusable
            voice-ba
        </div>
        <div>sed assistive technology termed the “Vocal
            Joystick” (VJ).
        </div>
        <div>The VJ approach has three main characteristics:
            1) Continuous control parameters: Unlike
        </div>
        <div>standard
            speech recognition, the VJ engine exploits continuous
            vocal characteristics that
        </div>
        <div> go beyond simple sequences
            of discrete speech sounds (such as syllables
            or words) and in
        </div>
        <div>clude e.g., pitch, vowel quality, and
            loudness, which are then mapped to continuous contr
        </div>
        <div>ol parameters.
            2) Discrete vocal commands: Unlike natural speech,
            the VJ discrete input l
        </div>
        <div>anguage is based on a predesigned
            set of sounds. These sounds are selected
            with respect t
        </div>
        <div>o acoustic discriminability (maximizing
            recognizer accuracy), pronounceability (reducing
        </div>
        <div>potential vocal strain), mnemonic characteristics
            (reducing cognitive load), robustness t
        </div>
        <div>o environmental
            noise, and application appropriateness.
            3) Reusable infrastructure: Our g
        </div>
        <div>oal is not to create
            a single application but to provide a modular library
            that can be in
        </div>
        <div>corporated by developers into a variety
            of applications that can be controlled by voice.
        </div>
        <div>The
            VJ technology is not meant to replace standard ASR
            but to enhance and be compatible w
        </div>
        <div>ith it.</div>
        <div>Three continuous vocal characteristics are extracted
            by the VJ engine: energy, pitch, and
        </div>
        <div> vowel quality,
            yielding four specifiable continuous degrees of
            freedom. The first of the
        </div>
        <div>se, localized acoustic energy,
            is used for voice activity detection. In addition,
            it is n
        </div>
        <div>ormalized relative to the current vowel
            detected (see Section 3.3), and is used by our cu
        </div>
        <div>rrent
            VJ-WIMP application (Section 4) to control the
            velocity of cursor movements. For ex
        </div>
        <div>ample, a loud
            voice causes a faster movement than does a quiet
            voice. The second paramete
        </div>
        <div>r, pitch, is also extracted
            but is currently not mapped to any control dimension
            in the V
        </div>
        <div>J-WIMP application but will be in the future.
            The third parameter is vowel quality. Unlik
        </div>
        <div>e consonants,
            which are characterized by a greater degree of
            constriction in the vocal tr
        </div>
        <div>act, vowels have much inherent
            signal energy and are therefore well-suited to
            environment
        </div>
        <div>s where both high accuracy and noiserobustness
            are crucial. Vowels can be characterized
            u
        </div>
        <div>sing a 2-D space parameterized by F1 and F2, the
            first and second vocal-tract formants (r
        </div>
        <div>esonant frequencies).
            We initially experimented with directly
            extracting F1/F2 and using
        </div>
        <div>them for directly specifying
            2-D continuous control. While we have not
            ruled out the use
        </div>
        <div>of F1/F2 in the future, we have
            so far found that even the best F1/F2 detection algorithm
        </div>
        <div>s
            available are not yet accurate enough for
            precise real-time specification of movement.
        </div>
        <div>Therefore,
            we classify vowels directly and map them onto
            the 2-D vowel space characterize
        </div>
        <div>d by degree of constriction
            (i.e., tongue height) and tongue body position
            (Figure 1). In
        </div>
        <div> our VJ-WIMP application, we use the four corners of this chart to map to the 4 principle</div>
        <div>
            directions of up, down, left, and right as shown
            in Figure 2 (note that the two figures
        </div>
        <div>are flipped and
            rotated with respect to each other). We have four
            different VJ systems ru
        </div>
        <div>nning: A) a 4-class system
            allowing only the specification of the 4 principle directions;
        </div>
        <div>
            B) a 5-class system that also includes the
            phone [ax] to act as a carrier when wishing t
        </div>
        <div>o vary
            only pitch and loudness; C) a 8-class system that includes
            the four diagonal direc
        </div>
        <div>tions; and D) a 9-class
            system that includes all phones and directions. Most
            of the discu
        </div>
        <div>ssion in this paper refers to the 4-class
            system.
            A fourth vocal characteristic is also e
        </div>
        <div>xtracted
            by the VJ engine, namely discrete sounds. These
            sounds may correspond to button
        </div>
        <div>presses as on a
            mouse or joystick. The choice of sounds depends
            on the application and ar
        </div>
        <div>e chosen according to characteristic
            2 above.
        </div>
        <div>Our system-level design goals are modularity, low
            latency, and maximal computational effi
        </div>
        <div>ciency. For
            this reason, we share common signal processing
            operations in multiple signal
        </div>
        <div>extraction modules,
            which yields real-time performance but leaves considerable
            computatio
        </div>
        <div>nal headroom for the back-end
            applications being driven by the VJ engine.
            Figure 3 shows
        </div>
        <div>the VJ engine architecture having
            three modules: signal processing, pattern recognition,
        </div>
        <div>and motion control.</div>
        <div>The goal of the signal processing module is to extract
            low-level acoustic features that c
        </div>
        <div>an be used in estimating the vocal characteristics. The features we
            use are energy, norma
        </div>
        <div>lized cross-correlation coefficients
            (NCCC), formant estimates, Mel-frequency
            cepstral co
        </div>
        <div>efficients (MFCCs), and formant estimates.
            To extract features, the speech signal is PCM
        </div>
        <div>sampled at a rate of Fs =16,000Hz. Energy is measured
            on a frame-by-frame basis with a fr
        </div>
        <div>ame size
            of 25ms and a frame step of 10ms. Pitch is extracted
            with a frame size of 40ms a
        </div>
        <div>nd a frame step of
            10ms. Multiple pattern recognition tasks may share
            the same acoustic f
        </div>
        <div>eatures: for example, energy and
            NCCCs are used for pitch tracking, and energy and
            MFCCs
        </div>
        <div>can be used in vowel classification and discrete
            sound recognition. Therefore, it is more
        </div>
        <div> efficient
            to decouple feature extraction from pattern
            recognition, as is shown in Figure
        </div>
        <div> 3.</div>
        <div>The pattern recognition module uses the acoustic
            features to extract desired parameters.
        </div>
        <div>The estimation
            and classification system must simultaneously
            perform energy computation (
        </div>
        <div>available from the input),
            pitch tracking, vowel classification, and discrete
            sound recog
        </div>
        <div>nition.
            Many state-of-the-art pitch trackers are based on
            dynamic programming (DP). This,
        </div>
        <div> however, often
            requires the meticulous design of local DP cost functions.
            The forms of t
        </div>
        <div>hese cost functions are usually
            empirically determined and/or their parameters
            are tuned
        </div>
        <div>by algorithms such as gradient descent
            (D.Talkin, 1995). Since different languages and ap
        </div>
        <div>plications
            may follow very different pitch transition
            patterns, the cost functions optimi
        </div>
        <div>zed for certain languages
            and applications may not be the most appropriate
            for others. Ou
        </div>
        <div>r VJ system utilizes a graphical
            model mechanism to automatically optimize the
            parameters
        </div>
        <div> of these cost functions, and has been
            shown to yield state-of-the-art performance (X.Li
        </div>
        <div>et
            al., 2004; J.Malkin et al., 2005).
            For frame-by-frame vowel classification, our design
        </div>
        <div>
            constraints are the need for extremely low latency
            and low computational cost. Probabili
        </div>
        <div>ty estimates
            for vowel classes thus need to be obtained
            as soon as possible after the vow
        </div>
        <div>el has been uttered
            or after any small change in voice quality has occurred.
            It is well k
        </div>
        <div>nown that models of vowel classification
            that incorporate temporal dynamics such
            as hidde
        </div>
        <div>n Markov models (HMMs) can be quite accurate.
            However, the frame-by-frame latency require
        </div>
        <div>ments
            of VJ make HMMs unsuitable for vowel
            classification since HMMs estimate the likelih
        </div>
        <div>ood
            of a model based on the entire utterance. An alternative
            is to utilize causal “HMM-fi
        </div>
        <div>ltering”, which
            computes likelihoods at every frame based on all
            frames seen so far. We h
        </div>
        <div>ave empirically found,
            however, that slightly non-causal and quite localized
            estimates of
        </div>
        <div> the vowel category probability
            is sufficient to achieve user satisfaction. Specifically,
        </div>
        <div>
            we obtain probability estimates of the form
            p(Vt|Xt− , . . . ,Xt+ ), where V is a vowe
        </div>
        <div>l class,
            and Xt− , . . . ,Xt+ are feature frames within a
            length 2 + 1 window of featu
        </div>
        <div>res centered at time
            t. After several empirical trials, we decided on
            neural networks for
        </div>
        <div> vowel classification because of
            the availability of efficient discriminative training al
        </div>
        <div>gorithms
            and their computational simplicity. Specifically
            we use a simple 2-layer multi-l
        </div>
        <div>ayer perceptron
            (Bishop, 1995) whose input layer consists of
            26  7 = 182 nodes, where 26
        </div>
        <div> is the dimension of
            Xt, the MFCC feature vector, and 2 + 1 = 7 is the number of consecu
        </div>
        <div>tive frames, and that has 50 hidden
            nodes (the numbers 7 and 50 were determined
            empirical
        </div>
        <div>ly). The output layer has 4 output nodes
            representing 4 vowel probabilities. During train
        </div>
        <div>ing,
            the network is optimized to minimize the Kullback-
            Leibler (K-L) divergence between
        </div>
        <div>the output and the
            true label distribution, thus achieving the aforementioned
            probabilist
        </div>
        <div>ic interpretation.
            The VJ engine needs not only to detect that the
            user is specifying a v
        </div>
        <div>owel (for continuous control)
            but also a consonant-vowel-consonant (CVC) pattern
            (for dis
        </div>
        <div>crete control) quickly and with a low
            probability of confusion (a VJ system also uses C
            a
        </div>
        <div>nd CV patterns for discrete commands). Requiring
            an initial consonant will phonetically d
        </div>
        <div>istinguish
            these sounds from the pure vowel segments used
            for continuous control — the VJ
        </div>
        <div> system constantly
            monitors for changes that indicate the beginning of
            one of the discret
        </div>
        <div>e control commands. The vowel
            within the CV and CVC patterns, moreover, can help
            prevent
        </div>
        <div>background noise from being mis-classified
            as a discrete sound. Lastly, each such pattern
        </div>
        <div> currently
            requires an ending silence, so that the next
            command (a new discrete sound or
        </div>
        <div>continuous control
            vowel) can be accurately initiated. In all cases, a
            simple threshold-b
        </div>
        <div>ased rejection mechanism is used
            to reduce false positives.
            To recognize the discrete con
        </div>
        <div>trol signals, HMMs
            are employed since, as in standard speech recognition,
            time warping is
        </div>
        <div> necessary to normalize for different
            signal durations corresponding to the same
            class. S
        </div>
        <div>pecifically, we embed phone HMMs into
            “word” (C, CV, or CVC) HMMs. In this way, it
            is pos
        </div>
        <div>sible to train phone models using a training
            set that covers all possible phones, and the
        </div>
        <div>n construct
            an application-specific discrete command vocabulary
            without retraining by rec
        </div>
        <div>ombining existing
            phone HMMs into new word HMMs. Therefore,
            each VJ-driven application ca
        </div>
        <div>n have its own appropriate
            discrete sound set.
        </div>
        <div>The VJ motion control module receives several pattern
            recognition parameters and processe
        </div>
        <div>s them to
            produce output more appropriate for determining 2-
            D movement in the VJ-WIMP ap
        </div>
        <div>plication.
            Initial experiments suggested that using pitch to
            affect cursor velocity (Igar
        </div>
        <div>ashi and Hughes, 2001)
            would be heavily constrained by an individual’s vocal vocal
            range.
        </div>
        <div> Giving priority to a more universal userindependent
            VJ system, we instead focused on rel
        </div>
        <div>ative
            energy. Our observation that users often became
            quiet when trying to move small amo
        </div>
        <div>unts confirmed
            energy as a natural choice. Drastically different intrinsic
            average energy
        </div>
        <div> levels for each vowel, however,
            meant that comparing all sounds to a global average
            ener
        </div>
        <div>gy would create a large vowel-dependent
            bias. To overcome this, we distribute the energy
        </div>
        <div>per
            frame among the different vowels, in proportion to
            the probabilities output by the ne
        </div>
        <div>ural network, and
            track the average energy for each vowel independently.
            By splitting the
        </div>
        <div> power in this way, there is
            no effect when probabilities are close to 1, and we
            smooth o
        </div>
        <div>ut changes during vowel transitions when
            probabilities are more evenly distributed.
            There
        </div>
        <div> are many possible options for determining
            velocity (a vector capturing both direction an
        </div>
        <div>d speed
            magnitude) and “acceleration” (a function determining
            how the control-to-display
        </div>
        <div>ratio changes based
            on input parameters), and the different schemes have
            a large impact o
        </div>
        <div>n user satisfaction. Unlike a standard
            mouse cursor, where the mapping is from 2-D hand
            m
        </div>
        <div>ovement to a 2-D screen, the VJ system maps from
            vocal-tract articulatory movement to a 2
        </div>
        <div>-D screen,
            and the transformation is not as straightforward. All
            values are for the curre
        </div>
        <div>nt time frame t unless indicated
            otherwise. First, a raw direction value is calculated
            fo
        </div>
        <div>r each axis j 2 {x, y} as
            dj =
            X
            i
            pi · hvi, eji (1)
            in which pi = p(Vt = i|Xt−,...,t+
        </div>
        <div>) is the probability
            for vowel i at time t, vi is a unit vector in the
            direction of vowel
        </div>
        <div> i, ej is the unit-length positive directional
            basis vector along the j axis, and hv, ei
        </div>
        <div>is
            the projection of vector v onto unit vector e. To determine
            movement speed, we first c
        </div>
        <div>alculate a scalar
            for each axis j as
            sj =
            X
            i
            max
            
            0, gi
            
            pi · f(E
            μi
            )
            
            · |hvi, ej i
        </div>
        <div>|
            where E is the energy in the current frame, μi is the
            average energy for vowel i, and f
        </div>
        <div>(·) and gi(·) are
            functions used for energy normalization and perceptual
            scaling (such as
        </div>
        <div> logs and/or cube-roots). This
            therefore allocates frame energy to direction based
            on the
        </div>
        <div> vowel probabilities. Lastly, we calculate the
            velocity for axis j at the current frame a
        </div>
        <div>s
            Vj = · s
            j · exp(
            sj). where represents the overall system sensitivity and
            the oth
        </div>
        <div>er values ( and
            ) are warping constants, allowing
            the user to control the shape of the a
        </div>
        <div>cceleration
            curve. Typically only one of  and
            is nonzero.
            Setting both to zero results
        </div>
        <div>in constant-speed movement
            along each axis, while  = 1 and
            = 0
            gives a linear mapping t
        </div>
        <div>hat will scale motion with
            energy but have no acceleration. The current userindependent
            s
        </div>
        <div>ystem uses = 0.6,
            = 1.0 and sets
             = 0. Lastly, the final velocity along axis j is Vjd
        </div>
        <div>j .
            Future publications will report on systematic evaluations
            of different f(·) and gi(·)
        </div>
        <div> functions</div>
        <div>Since vowel quality is used for continuous control,
            inaccuracies can arise due to speaker
        </div>
        <div> variability owing
            to different speech loudness levels, vocal tract
            lengths, etc. Moreove
        </div>
        <div>r, a vowel class articulated by
            one user might partially overlap in acoustic space
            with a
        </div>
        <div> different vowel class from another user. This
            imposes limitations on a purely user-indep
        </div>
        <div>endent
            vowel classifier. Differences in speaker loudness
            alone could cause significant un
        </div>
        <div>predictability. To
            mitigate these problems, we have designed an adaptation
            procedure wher
        </div>
        <div>e each user is asked to pronounce
            four pre-defined vowel sounds, each lasting
            2-3 seconds
        </div>
        <div>, at the beginning of a VJ session.
            We have investigated several novel adaptation
            strateg
        </div>
        <div>ies utilizing both neural networks and support
            vector machines (SVM). The fundamental ide
        </div>
        <div>a behind
            them both is that an initial speaker-independent
            transformation of the space is
        </div>
        <div>learned using training
            data, and is represented by the first layer of a
            neural network. A
        </div>
        <div>daptation data then is used to
            transform various parameters of the classifier (e.g.,
            all
        </div>
        <div>or sub-portions of the neural network, or the parameters
            of the SVM). Further details of
        </div>
        <div>some of these
            novel adaptation strategies appear in (X.Li et al.,
            2005), and the remainde
        </div>
        <div>r will appear in forthcoming
            publications. Also, the average energy values of
            each vowel
        </div>
        <div>for each user are recorded and used to
            normalize the speed control rate mentioned above.
        </div>
        <div>Preliminary evaluations on the data so far collected
            show very good results, with adaptat
        </div>
        <div>ion reducing the
            vowel classification error rate by 18% for the 4-class
            case, and 35% for
        </div>
        <div> the 8-class case. Moreover, informal
            studies have shown that users greatly prefer the
            VJ
        </div>
        <div> system after adaptation than before.</div>
        <div>Our overall intent is for VJ to interface with a variety
            of applications, and our primary
        </div>
        <div> application
            so far has been to drive a standard WIMP interface
            with VJ controls, what we
        </div>
        <div> call the VJ-WIMP application.
            The current VJ version allows left button
            clicks (press an
        </div>
        <div>d release, using the consonant
            [k]) as well as left button toggles (using consonant
            [ch])
        </div>
        <div> to allow dragging. Since WIMP interfaces
            are so general, this allows us to indirectly co
        </div>
        <div>ntrol
            a plethora of different applications. Video demonstrations
            are available at the URL
        </div>
        <div>: http://ssli.
            ee.washington.edu/vj.
            One of our key VJ applications is vocal web
            browsing
        </div>
        <div>. The video (dated 6/2005) shows examples
            of two web browsing tasks, one as an example
            of
        </div>
        <div> navigating the New York Times web site, the
            other using Google Maps to select and zoom i
        </div>
        <div>n on a
            target area. Section 5 describes a preliminary evaluation
            on these tasks. We have
        </div>
        <div>also started using the
            VJ engine to control video games (third video example),
            have inter
        </div>
        <div>faced VJ with the Dasher system
            (Ward et al., 2000) (we call it the “Vocal Dasher”),
            and
        </div>
        <div>have also used VJ for figure drawing.
            Several additional direct VJ-applications have
            also
        </div>
        <div> been developed. Specifically, we have directly
            interfaced the VJ system into a simple bl
        </div>
        <div>ocks world
            environment, where more precise object movement
            is possible than via the mouse
        </div>
        <div> driver. Specifically,
            this environment can draw arbitrary trajectories, and
            can precisel
        </div>
        <div>y measure user fidelity when moving an
            object along a trajectory. Fidelity depends both o
        </div>
        <div>n
            positional accuracy and task duration. One use of
            this environment shows the spatial di
        </div>
        <div>rection corresponding
            to vocal effort (useful for training, forth
            video example). Another
        </div>
        <div> shows a simple robotic
            arm being controlled by VJ. We plan to use this
            environment to pe
        </div>
        <div>rform formal and precise userperformance
            studies in future work.
        </div>
        <div>of individuals with motor impairments, and: 2) the
            users had only a small amount of time
        </div>
        <div>to practice and
            become adept at using VJ, the study is still indicative
            of the VJ approac
        </div>
        <div>h’s overall viability as a novel
            voice-based human-computer interface method. The
            study q
        </div>
        <div>uantitatively compares VJ performance with
            a standard desktop mouse, and provides qualita
        </div>
        <div>tive
            measurement of the user’s perception of the system.
        </div>
        <div>We recruited seven participants ranging from age 22
            to 26, none of whom had any motor imp
        </div>
        <div>airment.
            Of the seven participants, two were female and five
            were male. All of them were
        </div>
        <div>graduate students in
            Computer Science, although none of them had previously
            heard of or u
        </div>
        <div>sed VJ. Four of the participants
            were native English speakers; the other three had an
            Asi
        </div>
        <div>an language as their mother tongue.
            We used a Dell Inspiron 9100 laptop with a 3.2
            GHz In
        </div>
        <div>tel Pentium IV processor running the Fedora
            Core 2 operating system, with a 1280 x 800 24
        </div>
        <div>-bit
            color display. The laptop was equipped with an external
            Microsoft IntelliMouse conne
        </div>
        <div>cted through the
            USB port which was used for all of the tasks involving
            the mouse. A head
        </div>
        <div>-mounted Amanda NC-
            61 microphone was used as the audio input device,
            while the audio fee
        </div>
        <div>dback from the laptop was output
            through the laptop speakers. The Firefox browser
            was use
        </div>
        <div>d for all of the tasks, with the browser screen
            maximized such that the only portion of t
        </div>
        <div>he screen
            which was not displaying the contents of the web
            page was the top navigation to
        </div>
        <div>olbar which was 30
            pixels high.
        </div>
        <div>At the beginning of the quantitative evaluation, each
            participant was given a brief descr
        </div>
        <div>iption of the VJ
            operations and was shown a demonstration of the
            system by a practiced ex
        </div>
        <div>perimenter. The participant
            was then guided through an adaptation process during
            which sh
        </div>
        <div>e/he was asked to pronounce the four
            directional vowels (Section 3.4). After adaptation,
        </div>
        <div>the participant was given several minutes to practice
            using a simple target clicking appl
        </div>
        <div>ication. The quantitative
            portion of our evaluation followed a withinparticipant
            design.
        </div>
        <div>We exposed each participant to
            two experimental conditions which we refer to as
            input mod
        </div>
        <div>alities: the mouse and the VJ. Each participant
            completed two tasks on each modality, wit
        </div>
        <div>h
            one trial per task.
            The first task was a link navigation task (Link),
            in which the part
        </div>
        <div>icipants were asked to start from a
            specific web page and follow a particular set of link
        </div>
        <div>s
            to reach a destination. Before the trial, the experimenter
            demonstrated the specified s
        </div>
        <div>equence of links
            to the participant by using the mouse and clicking at
            the appropriate li
        </div>
        <div>nks. The participant was also provided
            with a sheet of paper for their reference that
            lis
        </div>
        <div>ted the sequence of links that would lead them to
            the target. The web site we used was a
        </div>
        <div>Computer
            Science Department student guide and the task involved
            following six links with
        </div>
        <div>the space between
            each successive link including both horizontal and
            vertical components.
        </div>
        <div>
            The second task was map navigation (Map), in
            which the participant was asked to navigate
        </div>
        <div> an online
            map application from a starting view (showing
            the entire USA) to get to a view
        </div>
        <div> showing a particular
            campus. The size of the map was 400x400
            pixels, and the set of avai
        </div>
        <div>lable navigation controls
            surrounding the map included ten discrete zoom
            level buttons, e
        </div>
        <div>ight directional panning arrows, and
            a click inside the map causing the map to be centere
        </div>
        <div>d
            and zoomed in by one level. Before the trial, a practiced
            experimenter demonstrated how
        </div>
        <div> to locate the
            campus map starting from the USA view to ensure
            they were familiar with th
        </div>
        <div>e geography.
            For each task, the participants performed one trial
            using the mouse, and one
        </div>
        <div> trial using a 4-class VJ.
            The trials were presented to the participants in a
            counterbala
        </div>
        <div>nced order. We recorded the completion
            time for each trial, as well as the number of fals
        </div>
        <div>e
            positives (system interprets a click when the user
            did not make a click sound), missed
        </div>
        <div>recognitions
            (the user makes a click sound but the system fails to
            recognize it as a clic
        </div>
        <div>k), and user errors (whenever
            the user clicks on an incorrect link). The recorded
            trial t
        </div>
        <div>imes include the time used by all of the above
            errors including recovery time.
            After the
        </div>
        <div>completion of the quantitative evaluation,
            the participants were given a questionnaire
            wh
        </div>
        <div>ich consisted of 14 questions related to the participants’
            perception of their experience
        </div>
        <div> using VJ such
            as the degree of satisfaction, frustration, and embarrassment.
            The answers
        </div>
        <div> were encoded on a 7-point
            Likert scale. We also included a space where the
            participants
        </div>
        <div>could write in any comments, and an informal post-experiment interview was performed to
            s
        </div>
        <div>olicit further feedback.</div>
        <div>Figure 4 shows the task completion times for Link
            and Map tasks, Figure 5 shows the break
        </div>
        <div>down of
            click errors by individual participants, Figure 6
            shows the average number of fal
        </div>
        <div>se positive and
            missed recognition errors for each of the tasks.
            There was no instance of
        </div>
        <div> user error in any trial. Figure
            7 shows the median of the responses to each of
            the fourt
        </div>
        <div>een questionnaire questions (error bars in
            each plot show ± standard error). In our measu
        </div>
        <div>rement
            of the task completion times, we considered
            the VJ’s recognition error rate as a f
        </div>
        <div>ixed factor, and
            thus did not subtract the time spent during those errors
            from the task c
        </div>
        <div>ompletion time.
            There were several other interesting observations
            that were made througho
        </div>
        <div>ut the study. We noticed
            that the participants who had the least trouble with
            missed reco
        </div>
        <div>gnitions for the clicking sound were either female or with an Asian language background,
        </div>
        <div>as shown in Figure 5. Our hypothesis regarding the
            better performance by female participa
        </div>
        <div>nts is that the
            original click sound was trained on one of our female
            researcher’s voice.
        </div>
        <div> We plan also in future work
            to determine how the characteristics of different native
            lan
        </div>
        <div>guage speakers influence VJ, and ultimately
            to correct for any bias.
            All but one user exp
        </div>
        <div>licitly expressed their confusion
            in distinguishing between the [ae] and [aa] vowels.
            Fou
        </div>
        <div>r of the seven participants independently
            stated that their performance would probably ha
        </div>
        <div>ve
            been better if they had been able to practice longer,
            and did not attribute their perc
        </div>
        <div>eived suboptimal performance
            to the quality of the VJ’s recognition system.
            Several parti
        </div>
        <div>cipants reported that they felt their
            vocal cords were strained due to having to produce
        </div>
        <div>a
            loud sound in order to get the cursor to move at the
            desired speed. We suspect this is
        </div>
        <div>due either to analog
            gain problems or to their adapted voice being too
            loud, and therefor
        </div>
        <div>e the system calibrating the normal
            speed to correspond to the loud voice. We have
            since
        </div>
        <div>removed this problem by adjusting our adaptation strategy to express preference for a qui</div>
        <div>et voice.
            In summary, the results from our study suggest
            that users without any prior exp
        </div>
        <div>erience were able
            to perform basic mouse based tasks using the Vocal
            Joystick system with
        </div>
        <div> relative slowdown of four to
            nine times compared to a conventional mouse. We
            anticipate
        </div>
        <div>that future planned improvements in the
            algorithms underlying the VJ engine (to improve a
        </div>
        <div>ccuracy,
            user-independence, adaptation, and speed)
            will further increase the VJ system’s
        </div>
        <div>viability, and
            combined with practice could improve VJ enough so
            that it becomes a reason
        </div>
        <div>able alternative compared to
            a standard mouse’s performance.
        </div>
        <div>Related voice-based interface studies include
            (Igarashi and Hughes, 2001; Olwal and Feine
        </div>
        <div>r,
            2005). Igarashi &amp; Hughes presented a system where
            non-verbal voice features control a
        </div>
        <div>mouse system—
            their system requires a command-like discrete sound
            to determine direction
        </div>
        <div>before initiating a movement
            command, where pitch is used to control velocity.
            We have em
        </div>
        <div>pirically found an energy-based
            mapping for velocity (as used in our VJ system)
            both more
        </div>
        <div> reliable (no pitch-tracking errors) and
            intuitive. Olwal &amp; Feiner’s system moves the mou
        </div>
        <div>se
            only after recognizing entire words. de Mauro’s
            “voice mouse” http://www.dii.unisi.it/
        </div>
        <div>
            maggini/research/voice mouse.html
            focuses on continuous cursor movements similar
            to the
        </div>
        <div> VJ scenario; however, the voice mouse
            only starts moving after the vocalization has been
        </div>
        <div>
            completed leading to long latencies, and it is not
            easily portable to other applications
        </div>
        <div>. Lastly, the
            commercial dictation program Dragon by ScanSoft
            includes MouseGridTM(Dra, 2
        </div>
        <div>004) which allows
            discrete vocal commands to recursively 9-partition
            the screen, thus ach
        </div>
        <div>ieving log-command access to a
            particular screen point. A VJ system, by contrast,
            uses co
        </div>
        <div>ntinuous aspects of the voice, has change
            latency (about 60ms) not much greater than reac
        </div>
        <div>tion
            time, and allows the user to make instantaneous
            directional change using one’s voice
        </div>
        <div> (e.g., a user
            can draw a ”U” shape in one breath).
        </div>
        <div>We have presented new voice-based assistive technology
            for continuous control tasks and h
        </div>
        <div>ave demonstrated an initial system implementation of
            this concept. An initial user study
        </div>
        <div>using a group
            of individuals from the non-target population confirmed
            the feasibility of
        </div>
        <div>this technology. We plan
            next to further improve our system by evaluating a
            number of nov
        </div>
        <div>el pattern classification techniques to
            increase accuracy and user-independence, and to i
        </div>
        <div>ntroduce
            additional vocal characteristics (possibilities
            include vibrato, degree of nasal
        </div>
        <div>ity, rate of change
            of any of the above as an independent parameter)
            to increase the avai
        </div>
        <div>lable simultaneous degrees of
            freedom controllable via the voice. Moreover, we
            plan to de
        </div>
        <div>velop algorithms to decouple unintended
            user correlations of these parameters, and to fur
        </div>
        <div>ther
            advance both our adaptation and acceleration algorithms.
        </div>
        <div id="wrapper_notes"></div>
    </div>


    <h1>Modify Configuration:</h1>

    <h2>Select Key to Modify:</h2>
    <form id="modify-form">
        <label for="config-keys-dropdown">Key:</label>
        <select id="config-keys-dropdown" required></select><br><br>
        <label for="new-value">New Value:</label>
        <input type="text" id="new-value" name="new-value" required><br><br>
        <button type="button" onclick="modifyConfig()">Modify</button>
    </form>

    <h2>Current Configuration:</h2>
    <pre id="config-content"></pre>

    <!-- JavaScript code -->
    <script>
        // Set the worker source for PDF.js library
        pdfjsLib.GlobalWorkerOptions.workerSrc = "https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.4.120/pdf.worker.min.js";

        // Get references to various elements
        let pdfinput = document.querySelector(".selectpdf"); // Reference to the PDF file input field
        let pwd = document.querySelector(".pwd"); // Reference to the password input field
        let upload = document.querySelector(".upload"); // Reference to the upload button
        let afterupload = document.querySelector(".afterupload"); // Reference to the result section
        let select = document.querySelector("select"); // Reference to the page selection dropdown
        let download = document.querySelector(".download"); // Reference to the download link
        let pdftext = document.querySelector(".pdftext"); // Reference to the text area for displaying extracted text

        // Event listener for the upload button click
        upload.addEventListener('click', () => {
            let file = pdfinput.files[0]; // Get the selected PDF file
            console.log(file);
            if (file != undefined && file.type == "application/pdf") {
                let fr = new FileReader(); // Create a new FileReader object
                fr.readAsDataURL(file); // Read the file as data URL
                fr.onload = () => {
                    let res = fr.result; // Get the result of file reading
                    if (pwd.value == "") {
                        extractText(res, false); // Extract text without password
                    } else {
                        extractText(res, true); // Extract text with password
                    }
                }
            } else {
                alert("Select a valid PDF file");
            }
        });

        let alltext = []; // Array to store all extracted text

        // Asynchronous function to extract text from the PDF
        async function extractText(url, pass) {
            try {
                let pdf;
                if (pass) {
                    pdf = await pdfjsLib.getDocument({url: url, password: pwd.value}).promise; // Get the PDF document with password
                } else {
                    pdf = await pdfjsLib.getDocument(url).promise; // Get the PDF document without password
                }
                let pages = pdf.numPages; // Get the total number of pages in the PDF
                for (let i = 1; i <= pages; i++) {
                    let page = await pdf.getPage(i); // Get the page object for each page
                    let txt = await page.getTextContent(); // Get the text content of the page
                    let text = txt.items.map((s) => s.str).join(""); // Concatenate the text items into a single string
                    alltext.push(text); // Add the extracted text to the array
                }
                alltext.map((e, i) => {
                    select.innerHTML += `<option value="${i + 1}">${i + 1}</option>`; // Add options for each page in the page selection dropdown
                });
                afterProcess(); // Display the result section
            } catch (err) {
                alert(err.message);
            }
        }


        function divideTesto(allText, targetDiv, maxCharsPerDiv = 89) {
            const textDiv = document.getElementById(targetDiv);
            textDiv.innerHTML = ''; // Pulisci il div prima di aggiungere nuovi elementi

            let currentText = '';
            for (let i = 0; i < allText[0].length; i++) {
                currentText += allText[0][i];
                if (currentText.length === maxCharsPerDiv) {
                    const newDiv = document.createElement('div');
                    newDiv.textContent = currentText;
                    textDiv.appendChild(newDiv);
                    currentText = '';
                }
            }
        }

        // Function to handle the post-processing after text extraction
        function afterProcess() {
            pdftext.value = alltext[select.value - 1]; // Display the extracted text for the selected page
            download.href = "data:text/plain;charset=utf-8," + encodeURIComponent(alltext[select.value - 1]); // Set the download link URL for the extracted text
            afterupload.style.display = "flex"; // Display the result section
            document.querySelector(".another").style.display = "unset"; // Display the "Extract Another PDF" button

            //Scrivo nelle mie div
            const textDiv = document.getElementById('target-div');

            //for (const text of alltext) {
            /*
                const paragraph = document.createElement('p');
                paragraph.innerHTML= alltext
                currentSentence=""
            */
            divideTesto(alltext, "target-div");
            /*
            for (let i = 0; i < alltext.length; i++) {
                const currentChar = alltext[i];

                // Append current character to the current sentence
                currentSentence += currentChar;

                if(alltext.length%89===0){
                    textDiv.appendChild(currentSentence);
                    currentSentence=""
                }

            }
            */

            //textDiv.appendChild(paragraph);
            //}

        }
    </script>

    <script src="vocal_command.js"></script>

</body>
</html>