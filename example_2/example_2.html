<!DOCTYPE html>
<html>

<head>
    <style>
        .piccolo {
            font-size: 12px;
        }
    </style>
    <!-- Set the character encoding to UTF-8 -->
    <meta charset="UTF-8">
    <!-- Specify the compatibility mode for Internet Explorer -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <!-- Set the viewport to control the layout on mobile devices -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Set the title of the web page -->
    <title>PDF voice notebook </title>
    <!-- Include the PDF.js library -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.4.120/pdf.min.js"
        integrity="sha512-ml/QKfG3+Yes6TwOzQb7aCNtJF4PUyha6R3w8pSTo/VJSywl7ZreYvvtUso7fKevpsI+pYVVwnu82YO0q3V6eg=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <!-- Add some styling for the web page -->
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <!-- Display the heading -->
    <h1>PDF voice notebook:</h1>
    <!-- Create a div container for the file upload form and result section -->
    <div class="pdfwork">
        <!-- Button to extract another PDF (hidden initially) -->
        <button class="another" onclick="location.reload()">Extract Another PDF</button>
        <!-- Display text "Select PDF" -->
        <span>Select PDF</span>
        <!-- File input field for selecting the PDF file -->
        <input type="file" class="selectpdf">
        <!-- Display text "Password :" -->
        <span>Password :</span>
        <!-- Password input field (optional) -->
        <input type="password" class="pwd" placeholder='optional'>
        <!-- Button to upload the selected PDF -->
        <button class="upload">Upload</button>
        <!-- Result section (hidden initially) -->
        <div class="afterupload">
            <!-- Display text "Select Page" -->
            <span style="display: none;">Select Page</span>
            <!-- Dropdown menu for selecting the page -->
            <select class="selectpage" onchange="afterProcess()" style="display: none;"></select>
            <!-- Download link for the extracted text file -->
            <a href="" style="display: none;" class="download" download>Download Pdf Text</a>
            <!-- Textarea to display the extracted text -->
            <textarea style="display: none;" class="pdftext"></textarea>
        </div>
    </div>


    <div id="outputElement"></div>
    <div class="container">
        <div id="target-div">
            <div>Abstract. This paper, firstly, introduces the application trend of the integration of mul</div>
            <div>ti-channel interactions in automotive HMI (Human Machine Interface) from complex informat</div>
            <div>ion models faced by existing automotive HMI and describes various interaction modes. By c</div>
            <div>omparing voice interaction and touch screen, gestures and other interaction modes, the po</div>
            <div>tential and feasibility of voice interaction in automotive HMI experience design are conc</div>
            <div>luded. Then, the related theories of voice interaction, identification technologies, huma</div>
            <div>n beings’ cognitive models of voices and voice design methods are further explored. And t</div>
            <div>he research priority of this paper is proposed, i.e. how to design voice interaction to c</div>
            <div>reate more humane task-oriented dialogue scenarios to enhance interactive experiences of </div>
            <div>automotive HMI. The specific scenarios in driving behaviors suitable for the use of voice</div>
            <div> interaction are studied and classified, and the usability principles and key elements fo</div>
            <div>r automotive HMI voice design are proposed according to the scenario features. Then, thro</div>
            <div>ugh the user participatory usability testing experiment, the dialogue processes of voice </div>
            <div>interaction in automotive HMI are defined. The logics and grammars in voice interaction a</div>
            <div>re classified according to the experimental results, and the mental models in the interac</div>
            <div>tion processes are analyzed. At last, the voice interaction design method to create the h</div>
            <div>umane task-oriented dialogue scenarios in the driving environment is proposed. Keywords: </div>
            <div>voice interaction; Automotive Human Machine Interface; driving experience; task-oriented </div>
            <div>dialogue scenario. </div>
            <div>Automotive intelligent interconnection and automatic driving are creating new interaction</div>
            <div>s and experiences between drivers and cars, and people are looking forward to making ever</div>
            <div>y trip a wonderful memory. Functions provided by high-quality experience HMI will no long</div>
            <div>er be simple piling up of isolated and unrelated functional modules. The future HMI desig</div>
            <div>n should be more based on user scenarios to establish the mutual linkage and intercommuni</div>
            <div>cations between functions inside cars and provide the reasonable function jumping, and al</div>
            <div>so based on scenario needs, choose and employ appropriate technologies and interaction mo</div>
            <div>des, minimize the degree of distraction of drivers and maximize the information efficienc</div>
            <div>y of the input and output data, allowing users to complete operational tasks efficiently,</div>
            <div> easily and pleasantly. And voice interaction, as the most competitive entry in the Inter</div>
            <div>net of Things era, creates brand new companion scenarios. In the driving environment, ful</div>
            <div>ly grasping the feature that it’s inconvenient for people to use hands and eyes, voice in</div>
            <div>teraction will not distract drivers and will not require too many efforts either to achie</div>
            <div>ve simple operations, accurate operations and safe driving. At present, more and more aut</div>
            <div>omotive manufacturers are actively researching and developing their own intelligent voice</div>
            <div> products to provide more options for voice interaction in driving experiences. But only </div>
            <div>a few high-end cars integrate voice interaction technologies into their vehicle-mounted s</div>
            <div>ystems. Therefore, researches on the application of voice interaction in automotive HMI e</div>
            <div>xperience design are the important direction for the future automotive HMI development.</div>
            <div>Human machine interaction in HMI, in some sense, can be interpreted as driving tasks exec</div>
            <div>ution. In various driving behaviors, keeping a normal driving and monitoring road hazards</div>
            <div> are the main tasks. Other tasks that require visual resources, such as radio operation o</div>
            <div>r telephone dialing can be seen as secondary driving tasks in a car. The major difference</div>
            <div> between different driving tasks lies in the visual operation and manual operation demand</div>
            <div> degree in the interactive process. From driving safety and operational accessibility per</div>
            <div>spective, pure manual operation or manual operation dominated task is the most popular on</div>
            <div>e, which can make the driver concentrates on finishing first level driving task in a mini</div>
            <div>mum range of visual distraction. However, the development of information entertainment sy</div>
            <div>stem leads to a large increase of visual oriented operational tasks. The information mode</div>
            <div>l inside the car (Fig 1) has been gradually developed from a single automobile condition </div>
            <div>information model into a complex information system, including car information, Car2Car i</div>
            <div>nformation, Car2X interactive information. Under such a complex information system, the c</div>
            <div>hallenge facing automotive HMI experience design is how to provide the driver with a bett</div>
            <div>er interactive experience in the process of human machine interaction in addition to ensu</div>
            <div>ring driving safety.</div>
            <div>The essence of human machine interaction in automotive HMI design is information transmis</div>
            <div>sion and processing. In the future, automotive HMI design will be based on scenario task,</div>
            <div> considering the application and cooperation of different interactive channels. In the pr</div>
            <div>ocess of task control, a certain interactive channel can be used as a main channel, combi</div>
            <div>ning with another interactive channel, such as voice+gesture or voice+button, to give ful</div>
            <div>l play of the interactive advantages of both channels, and smoothly complete the discrete</div>
            <div> control tasks and the continuous control tasks. Multichannel interactive interface integ</div>
            <div>rates voice interaction, touch screen interaction, space somatosensory interaction, eye m</div>
            <div>ovement interaction and other various interactive modes. It brings feedback to the users </div>
            <div>through multiple sensory channels, so as to provide more intuitive and natural interactio</div>
            <div>n experience. It reduces the burden of excessive visual and auditory information processi</div>
            <div>ng during driving, and balance the information into all sensory organs. </div>
            <div>With the development of technology and society, the voice interaction has created a brand</div>
            <div>-new syndrome scenario, and has become the most competitive entrance to the age of the in</div>
            <div>ternet of things. It helps people to communicate with machines in a natural mode of “chat</div>
            <div>”, without using complex physical controls or reading tedious rules, enables the machine </div>
            <div>to listen, speak, understand and think. Comparing the performance of touch, touch screen </div>
            <div>and voice interaction in three perspectives of driving performance, cognitive load and vi</div>
            <div>sual attention, we can summarize as follow (Fig 2 Non-quantitative description): it is th</div>
            <div>e commanding height of the development of big data and cognitive computing era in the fut</div>
            <div>ure. It has a broad market prospect and applicable significance. </div>

            <div>Nowadays, automotive voice interaction is increasingly popular in major brands. For insta</div>
            <div>nce, SAIC has launched the first intelligent voice cloud driving system iVoka in 2011.The</div>
            <div>y upgrade the system in 2012, that is the second generation iVoka system. It has been app</div>
            <div>lied to Roewe 350 and MG5, and put into market. The second generation iVoka system uses v</div>
            <div>oice command to replace traditional keystroke, it uses voice control system to dial or ha</div>
            <div>ng off the phone, receive or send short messages, navigate, inquire information, listen t</div>
            <div>o music and broadcast. Ford has released SYNC vehicle information entertainment system as</div>
            <div> early as 2007, and displayed the SYNC 3 system in International Consumer Electronics Exh</div>
            <div>ibition in 2015. SYNC 3 system supports the powerful voice control functions, try to avoi</div>
            <div>d driver distraction during system operation. After the connection of IPhone, SYNC 3 can </div>
            <div>be seamlessly connected with Siri, the driver only needs to press the button of speak, th</div>
            <div>en he can talk with Siri for help. At the same time, APP Link also supports voice control</div>
            <div>, it can search for applications compatible with the vehicle system and establish connect</div>
            <div>ions in the mobile phone, so as to realize the control of compatible applications through</div>
            <div> voice. SAIC and Ali launched the “zebra system” in 2017 on Roewe i6, although it did not</div>
            <div> make great progress like AlphaGo, it gave the driver a quite big change. Meanwhile, in M</div>
            <div>MC2016 (International Car Networking and Intelligent Transportation Exhibition), a forum </div>
            <div>themed as “Car Voice Control and Voice Life”, some well-known voice interaction solution </div>
            <div>providers delivered speech and made discussion, through which we can see that the applica</div>
            <div>tion of voice control technology in vehicle system has become an important part of the ve</div>
            <div>hicle network, voice interaction is gradually replacing traditional manual control, and b</div>
            <div>ecoming one of the significant features of intelligent driving. </div>
            <div>In the process of voice design, it is important to keep in mind the application purpose a</div>
            <div>nd user group corresponding to the voice instruction set. The most important factor affec</div>
            <div>ting the design of voice interaction in the development of HMI is: x The situation of voi</div>
            <div>ce application. Situation determines the content and form of the dialogue between people </div>
            <div>and machine. In the design process, we need detailed description of all roles, possible s</div>
            <div>ituations, scenarios and certain form of human machine information exchange ways in the p</div>
            <div>rocess of driving and interactive situation. Assume the possible tasks of the driver, and</div>
            <div> provide different communication content and form for the different elements and function</div>
            <div>al requirements. x Clarify the mapping functions and operational tasks. In fact, when use</div>
            <div>rs face different task goals, voice design is the process of constantly understanding of </div>
            <div>semantic function or operating task semantics. In the face of different task goals, users</div>
            <div> need more understanding of the effects of functional or operational tasks, so as to cons</div>
            <div>ider the form and expression of voice interaction. x Understanding the users and problem </div>
            <div>domain. In the process of voice design, psychological model and emotional state of the us</div>
            <div>er should be understood as much as possible. The feedback after the operation must meet t</div>
            <div>he expectation of the user. </div>
            <div>Although there is a considerable amount of studies on the application of intelligent voic</div>
            <div>e operation, most studies are still focusing on the hardware solution of voice interactio</div>
            <div>n, which leads to the lack of a clear design method and systematic framework of the voice</div>
            <div> interaction based automotive HMI, shown in Fig 3. The construction of automotive HMI mod</div>
            <div>ule based on voice interaction includes automotive HMI design, voice interaction input me</div>
            <div>thod, voice recognition technology, function mapping and voice operation based human mach</div>
            <div>ine interaction elements (visual guidance, display location, interaction feedback) and so</div>
            <div> on. </div>

            <div>If the quick, efficient, easy and natural features in the driving environment are the uni</div>
            <div>que advantages of voice interaction, then task-oriented function is the perfect foothold </div>
            <div>of these advantages. A good voice interactive product should be versatile, the better the</div>
            <div> more domain it can cover, the better the skills it can get. Once a certain scenario is d</div>
            <div>etermined to design, the voice interaction design can be divided into three steps: Clear </div>
            <div>Chat Flow, Design Grammer &amp; Design Confirmation. Step one: Chat Flow--where to come, wher</div>
            <div>e to go? As the same as graphical user interface, in which click-trigger is the interacti</div>
            <div>ve logic of each node, voice interaction also requires a flow logic from query to answer,</div>
            <div> which runs through the flow of a dialogue in a scenario. If the dialogue scenario you de</div>
            <div>signed is to query real-time traffic conditions, please consider any possible situations </div>
            <div>may occur in thi dialogue as well as corresponding reactions, then determine the granular</div>
            <div>ity of detail logic according to the importance of this scenario in human machine interac</div>
            <div>tion. </div>

            <div>Step 2: Design Grammer-- what will the user say to you? Grammer is the instruction sets i</div>
            <div>nput by the users, dialogue designer need to design the dialogue intent, try to take any </div>
            <div>possible ways of expression into account, and extract the most core and most common way o</div>
            <div>f expression as the template of instruction sets. The more and the more comprehensive the</div>
            <div> instruction sets are designed, the higher the dialogue coverage will be. To imagine scen</div>
            <div>ario or enquire about air quality, please consider what kind of expressive ways the user </div>
            <div>will use to make their own requests. The following figure 5 is the dialogue instruction s</div>
            <div>ets of air quality inquiry, in which “place” and “time” is the slot. Slot is the key info</div>
            <div>rmation point extracted from user instruction by NLU, NLU module understands the specific</div>
            <div> requirements of the user’s instructions through this key information and their value of </div>
            <div>definition(Slot-Value). </div>
            <div>Step 3: Design confirmation--how to answer the users The main confirmation way is TTS (Te</div>
            <div>xt to Speech), which is used to transform the confirmation script written by the designer</div>
            <div> into voice and play. Confirmation brings users the most intuitive feelings, the confirma</div>
            <div>tion is good or bad, directly related to the experience of voice products in the vehicle.</div>
            <div> Considering that a long voice content will increase the driver’s memory load, confirmati</div>
            <div>on design should be as concise as possible. Meanwhile, if the voice product in the car ha</div>
            <div>s its own character, please write the script in line with the language style to keep cons</div>
            <div>istency of the role, and fill in the blanks in the dialogue script file. “script” means w</div>
            <div>riting film scripts, and the whole process of dialogue design is really like designing fi</div>
            <div>lm scripts. </div>
            <div>Voice interaction can be used to quickly create a humanized task-oriented dialogue scenar</div>
            <div>io, the interactive form of questions and answers can help the drivers complete the task.</div>
            <div> More emotional design can be integrated into automotive HMI to express human concern and</div>
            <div> emotional interaction, increase the trust of users, and give the users more driving fun </div>
            <div>and emotional experience. </div>
            <div>The significance of this paper lies in its exploring the key design process of automotive</div>
            <div> HMI experience design based on voice interaction and the design method to create humane </div>
            <div>task-oriented dialogue scenarios in order to achieve the naturalness of interactions and </div>
            <div>integrate more emotional designs into HMI to demonstrate the humanity and emotional inter</div>
            <div>actions during the travelling process and grant users more driving pleasures and emotiona</div>
            <div>l experiences. This study provides some methodological guidance for the design of interac</div>
            <div>tion modes in automotive HMI, and is also a research and supplement for the user-oriented</div>
            <div> design concept. </div>
        </div>
        <div id="wrapper_notes"></div>
    </div>

    <h1>Modify Configuration:</h1>

    <h2>Select Key to Modify:</h2>
    <form id="modify-form">
        <label for="config-keys-dropdown">Key:</label>
        <select id="config-keys-dropdown" required></select><br><br>
        <label for="new-value">New Value:</label>
        <input type="text" id="new-value" name="new-value" required><br><br>
        <button type="button" onclick="modifyConfig()">Modify</button>
    </form>

    <h2>Current Configuration:</h2>
    <pre id="config-content"></pre>

    <!-- JavaScript code -->
    <script>
        // Set the worker source for PDF.js library
        pdfjsLib.GlobalWorkerOptions.workerSrc = "https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.4.120/pdf.worker.min.js";

        // Get references to various elements
        let pdfinput = document.querySelector(".selectpdf"); // Reference to the PDF file input field
        let pwd = document.querySelector(".pwd"); // Reference to the password input field
        let upload = document.querySelector(".upload"); // Reference to the upload button
        let afterupload = document.querySelector(".afterupload"); // Reference to the result section
        let select = document.querySelector("select"); // Reference to the page selection dropdown
        let download = document.querySelector(".download"); // Reference to the download link
        let pdftext = document.querySelector(".pdftext"); // Reference to the text area for displaying extracted text

        // Event listener for the upload button click
        upload.addEventListener('click', () => {
            let file = pdfinput.files[0]; // Get the selected PDF file
            console.log(file);
            if (file != undefined && file.type == "application/pdf") {
                let fr = new FileReader(); // Create a new FileReader object
                fr.readAsDataURL(file); // Read the file as data URL
                fr.onload = () => {
                    let res = fr.result; // Get the result of file reading
                    if (pwd.value == "") {
                        extractText(res, false); // Extract text without password
                    } else {
                        extractText(res, true); // Extract text with password
                    }
                }
            } else {
                alert("Select a valid PDF file");
            }
        });

        let alltext = []; // Array to store all extracted text

        // Asynchronous function to extract text from the PDF
        async function extractText(url, pass) {
            try {
                let pdf;
                if (pass) {
                    pdf = await pdfjsLib.getDocument({ url: url, password: pwd.value }).promise; // Get the PDF document with password
                } else {
                    pdf = await pdfjsLib.getDocument(url).promise; // Get the PDF document without password
                }
                let pages = pdf.numPages; // Get the total number of pages in the PDF
                for (let i = 1; i <= pages; i++) {
                    let page = await pdf.getPage(i); // Get the page object for each page
                    let txt = await page.getTextContent(); // Get the text content of the page
                    let text = txt.items.map((s) => s.str).join(""); // Concatenate the text items into a single string
                    alltext.push(text); // Add the extracted text to the array
                }
                alltext.map((e, i) => {
                    select.innerHTML += `<option value="${i + 1}">${i + 1}</option>`; // Add options for each page in the page selection dropdown
                });
                afterProcess(); // Display the result section
            } catch (err) {
                alert(err.message);
            }
        }


        function divideTesto(allText, targetDiv, maxCharsPerDiv = 89) {
            const textDiv = document.getElementById(targetDiv);
            textDiv.innerHTML = ''; // Pulisci il div prima di aggiungere nuovi elementi

            let currentText = '';
            for (let i = 0; i < allText[0].length; i++) {
                currentText += allText[0][i];
                if (currentText.length === maxCharsPerDiv) {
                    const newDiv = document.createElement('div');
                    newDiv.textContent = currentText;
                    textDiv.appendChild(newDiv);
                    currentText = '';
                }
            }
        }

        // Function to handle the post-processing after text extraction
        function afterProcess() {
            pdftext.value = alltext[select.value - 1]; // Display the extracted text for the selected page
            download.href = "data:text/plain;charset=utf-8," + encodeURIComponent(alltext[select.value - 1]); // Set the download link URL for the extracted text
            afterupload.style.display = "flex"; // Display the result section
            document.querySelector(".another").style.display = "unset"; // Display the "Extract Another PDF" button

            //Scrivo nelle mie div
            const textDiv = document.getElementById('target-div');

            //for (const text of alltext) {
            /*
                const paragraph = document.createElement('p');
                paragraph.innerHTML= alltext
                currentSentence=""
            */
            divideTesto(alltext, "target-div");
            /*
            for (let i = 0; i < alltext.length; i++) {
                const currentChar = alltext[i];
 
                // Append current character to the current sentence
                currentSentence += currentChar;
 
                if(alltext.length%89===0){
                    textDiv.appendChild(currentSentence);
                    currentSentence=""
                }
 
            }
            */

            //textDiv.appendChild(paragraph);
            //}

        }
    </script>

    <script src="vocal_command.js"></script>

</body>

</html>